{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Lower Level Python Binding API\n",
    "\n",
    "This notebook explains the features and capabilities of the Python binding for cuDNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/02_low_level_api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for running on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [introduction](00_introduction.ipynb), you saw how to create a graph and execute it with a Python binding directly. Let's see a different example of the same workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float16\n",
    "B, M, N, K = 16, 128, 128, 512\n",
    "\n",
    "# input tensors\n",
    "a_gpu = torch.randn(B, M, K, device=device, dtype=dtype)\n",
    "b_gpu = torch.randn(B, K, N, device=device, dtype=dtype)\n",
    "d_gpu = torch.randn(1, M, N, device=device, dtype=dtype)\n",
    "\n",
    "# place holder for cuDNN output\n",
    "c_gpu = torch.empty(B, M, N, device=device, dtype=dtype)\n",
    "\n",
    "# reference output\n",
    "c_ref = torch.matmul(a_gpu, b_gpu) + d_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle and construct the graph\n",
    "handle = cudnn.create_handle()\n",
    "graph = cudnn.pygraph(handle=handle, compute_data_type=cudnn.data_type.FLOAT)\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "d_cudnn = graph.tensor_like(d_gpu)\n",
    "\n",
    "ab = graph.matmul(name=\"mm\", A=a_cudnn, B=b_cudnn)\n",
    "ab.set_data_type(cudnn.data_type.HALF)\n",
    "c_cudnn = graph.bias(name=\"bias\", input=ab, bias=d_cudnn)\n",
    "c_cudnn.set_output(True).set_data_type(cudnn.data_type.HALF)\n",
    "\n",
    "# execute the graph\n",
    "graph.build([cudnn.heur_mode.A])\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=device, dtype=torch.uint8)\n",
    "variant_pack = {\n",
    "    a_cudnn: a_gpu,  # input\n",
    "    b_cudnn: b_gpu,  # input\n",
    "    d_cudnn: d_gpu,  # input\n",
    "    c_cudnn: c_gpu,  # output\n",
    "}\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes a batched matrix multiplication with bias: $C = A \\times B + D$. Tensor $A$ is a batch of $M\\times K$ matrices and tensor $B$ is a batch of $K\\times N$ matrices. Batch size is $B$. A constant bias matrix $D$ of size $M\\times N$ is added to each element of the output matrix $C$. The result $C$ is a batch of $M\\times N$ matrices.\n",
    "\n",
    "You created two nodes in the graph for this computation: One for the matrix-matrix multiplication and one for the bias addition. The variable `ab` is an intermediate tensor (i.e., virtual tensor) from the matrix-matrix multiplication and used as input to the bias addition. You need to set the data type of the intermediate tensor because the matrix-matrix multiplication node can output different data types.\n",
    "\n",
    "The graph output is `c_cudnn`. You mark it as non-virtual tensor by calling `set_output(True)`. You execute the graph by providing `variant_pack` as a dictionary mapping the cuDNN tensors you used in the graph to the allocated tensors you created with PyTorch. The output tensors will be updated in place when the graph executes.\n",
    "\n",
    "You can remove the `set_data_type()` calls to the intermediate tensor `ab` and output tensor `c_cudnn` if you set up the default in the `pygraph` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle and construct the graph\n",
    "handle = cudnn.create_handle()\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "d_cudnn = graph.tensor_like(d_gpu)\n",
    "\n",
    "ab = graph.matmul(name=\"mm\", A=a_cudnn, B=b_cudnn)\n",
    "c_cudnn = graph.bias(name=\"bias\", input=ab, bias=d_cudnn)\n",
    "c_cudnn.set_output(True)\n",
    "\n",
    "# execute the graph\n",
    "graph.build([cudnn.heur_mode.A])\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=device, dtype=torch.uint8)\n",
    "variant_pack = {\n",
    "    a_cudnn: a_gpu,  # input\n",
    "    b_cudnn: b_gpu,  # input\n",
    "    d_cudnn: d_gpu,  # input\n",
    "    c_cudnn: c_gpu,  # output\n",
    "}\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using decorators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use cuDNN as a decorator. The major benefit is that you can make the build step implicit. See below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_cache_key(handle, a, b, bias):\n",
    "    \"\"\"Custom key function for matmul + bias\"\"\"\n",
    "    return (\n",
    "        tuple(a.shape),\n",
    "        tuple(b.shape),\n",
    "        tuple(a.stride()),\n",
    "        tuple(b.stride()),\n",
    "        a.dtype,\n",
    "        b.dtype,\n",
    "    )\n",
    "\n",
    "\n",
    "@cudnn.jit(heur_modes=[cudnn.heur_mode.A, cudnn.heur_mode.B])\n",
    "@cudnn.graph_cache(key_fn=matmul_cache_key)\n",
    "def create_matmul_bias_graph(handle, a, b, bias):\n",
    "    with cudnn.graph(handle) as (g, _):\n",
    "        a_cudnn = g.tensor_like(a)\n",
    "        b_cudnn = g.tensor_like(b)\n",
    "        bias_cudnn = g.tensor_like(bias)\n",
    "        c_cudnn = g.matmul(name=\"matmul\", A=a_cudnn, B=b_cudnn)\n",
    "        out = g.bias(name=\"bias\", input=c_cudnn, bias=bias_cudnn)\n",
    "        out.set_output(True).set_data_type(cudnn.data_type.HALF)\n",
    "\n",
    "    return g, [a_cudnn, b_cudnn, bias_cudnn, out]  # Return raw graph and tensors\n",
    "\n",
    "\n",
    "g, uids = create_matmul_bias_graph(handle, a_gpu, b_gpu, d_gpu)\n",
    "a_uid, b_uid, bias_uid, out_uid = uids\n",
    "\n",
    "variant_pack = {\n",
    "    a_uid: a_gpu,\n",
    "    b_uid: b_gpu,\n",
    "    bias_uid: d_gpu,\n",
    "    out_uid: c_gpu,\n",
    "}\n",
    "workspace = torch.empty(g.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "g.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that in this example, you did not call `graph.build()`. Instead, you defined a function `create_matmul_bias_graph()` that returns a graph object and a list of cuDNN tensors. This part is same as the previous example. However, you decorated the function with `@cudnn.jit` to specify the heuristic modes to use to build the graph.\n",
    "\n",
    "You also decorated the function with `@cudnn.graph_cache` to specify a custom key function for the graph cache. The custom key function `matmul_cache_key()` depends on the shape, stride, and data type of the input tensors, but not the other attributes or the handle. This way, you can call this line multiple times without rebuilding the graph:\n",
    "\n",
    "```python\n",
    "g, uids = create_matmul_bias_graph(handle, a_gpu, b_gpu, d_gpu)\n",
    "```\n",
    "\n",
    "Note that when you called `create_matmul_bias_graph()`, you pass in a handle and multiple PyTorch tensors. The data held by the tensors are not important, as long as the data types and layouts are the same, the same graph will be returned by the cache. This could be a convenience for you because logically, this is a matrix multiplication with bias but the graph to use on cuDNN would be different depends on the floating point precision and the dimension of the input tensors. The graph cache helps you to keep track on the graph to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example above, you saw that to prepare a graph for execution, you need to call `graph.build()` and pass in a list of heuristic modes. This is indeed a meta-function that combines multiple steps into one. Below is an example to break down the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle and construct the graph\n",
    "handle = cudnn.create_handle()\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "d_cudnn = graph.tensor_like(d_gpu)\n",
    "\n",
    "ab = graph.matmul(name=\"mm\", A=a_cudnn, B=b_cudnn)\n",
    "c_cudnn = graph.bias(name=\"bias\", input=ab, bias=d_cudnn)\n",
    "c_cudnn.set_output(True)\n",
    "\n",
    "# build and validate the graph\n",
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()\n",
    "\n",
    "# execute the graph\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=device, dtype=torch.uint8)\n",
    "variant_pack = {\n",
    "    a_cudnn: a_gpu,  # input\n",
    "    b_cudnn: b_gpu,  # input\n",
    "    d_cudnn: d_gpu,  # input\n",
    "    c_cudnn: c_gpu,  # output\n",
    "}\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you break down the build process into multiple steps. This can give you more control over the actual graph execution plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization and Deserialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph created can be serialized and deserialized. This can save the overhead of rebuilding the graph from scratch. Below is an example of how to serialize a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle and construct the graph\n",
    "handle = cudnn.create_handle()\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "d_cudnn = graph.tensor_like(d_gpu)\n",
    "a_cudnn.set_uid(0)\n",
    "b_cudnn.set_uid(1)\n",
    "d_cudnn.set_uid(2)\n",
    "\n",
    "ab = graph.matmul(name=\"mm\", A=a_cudnn, B=b_cudnn)\n",
    "c_cudnn = graph.bias(name=\"bias\", input=ab, bias=d_cudnn)\n",
    "c_cudnn.set_output(True).set_uid(3)\n",
    "\n",
    "# validate the graph and serialize it\n",
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()\n",
    "serialized_data = graph.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, you created a graph, build the execution plan, and serialized it. The serialized data is a list of integers representing a byte stream. You can save the serialized data and reuse it later. Here is how you can deserialize the graph and execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new graph from serialized data\n",
    "newgraph = cudnn.pygraph()\n",
    "newgraph.deserialize(serialized_data)\n",
    "\n",
    "# execute the graph\n",
    "workspace = torch.empty(newgraph.get_workspace_size(), device=device, dtype=torch.uint8)\n",
    "variant_pack = {\n",
    "    0: a_gpu,  # input\n",
    "    1: b_gpu,  # input\n",
    "    2: d_gpu,  # input\n",
    "    3: c_gpu,  # output\n",
    "}\n",
    "newgraph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the result\n",
    "c_ref = torch.matmul(a_gpu, b_gpu) + d_gpu\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, you created a new `pygraph` object and populated it with the serialized data. Then you can execute the graph immediately without validation or building execution plan.\n",
    "\n",
    "You can see that this example has one major difference from the previous example: The cuDNN tensors involved are set with a particular UID. All tensors will be assigned a UID when the corresponding graph is built, but you can manually set the UID with `set_uid()` as long as the UID are unique. The reason you need to set the UID in this example is that the original `graph` object and the new `newgraph` object are distinct. To pass on the `variant_pack` to execute the new graph, you need to reference to the tensors in the new graph, which is not possible since you created the graph by deserialization. Therefore, you need to use tensors' UIDs instead.\n",
    "\n",
    "Let's see another example: A graph of a single SDPA operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2  # batch size\n",
    "s_q = 1024  # query sequence length\n",
    "s_kv = 1024  # key+value sequence length\n",
    "h = 6  # query heads\n",
    "d = 64  # query+key embedding dimension per head\n",
    "attn_scale = 1 / d**0.5\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "shape_q = (b, h, s_q, d)\n",
    "shape_k = shape_v = (b, h, s_kv, d)\n",
    "shape_o = (b, h, s_q, d)\n",
    "\n",
    "stride_q = stride_o = (s_q * h * d, d, h * d, 1)\n",
    "stride_k = stride_v = (s_kv * h * d, d, h * d, 1)\n",
    "\n",
    "# allocate PyTorch tensors as input and output\n",
    "q_gpu = torch.randn(b * h * s_q * d, dtype=dtype, device=\"cuda\").as_strided(\n",
    "    shape_q, stride_q\n",
    ")\n",
    "k_gpu = torch.randn(b * h * s_kv * d, dtype=dtype, device=\"cuda\").as_strided(\n",
    "    shape_k, stride_k\n",
    ")\n",
    "v_gpu = torch.randn(b * h * s_kv * d, dtype=dtype, device=\"cuda\").as_strided(\n",
    "    shape_v, stride_v\n",
    ")\n",
    "o_gpu = torch.empty(b * h * s_q * d, dtype=dtype, device=\"cuda\").as_strided(\n",
    "    shape_o, stride_o\n",
    ")\n",
    "stats_gpu = torch.empty(b, h, s_q, 1, dtype=dtype, device=\"cuda\")\n",
    "\n",
    "# define a graph\n",
    "graph = cudnn.pygraph(\n",
    "    io_data_type=cudnn.data_type.BFLOAT16,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    handle=handle,\n",
    ")\n",
    "\n",
    "q = graph.tensor_like(q_gpu)\n",
    "k = graph.tensor_like(k_gpu)\n",
    "v = graph.tensor_like(v_gpu)\n",
    "\n",
    "o, stats = graph.sdpa(\n",
    "    name=\"sdpa\",\n",
    "    q=q,\n",
    "    k=k,\n",
    "    v=v,\n",
    "    generate_stats=True,\n",
    "    attn_scale=attn_scale,\n",
    "    use_causal_mask=True,\n",
    ")\n",
    "q.set_uid(0)\n",
    "k.set_uid(1)\n",
    "v.set_uid(2)\n",
    "o.set_uid(3)\n",
    "stats.set_uid(4)\n",
    "o.set_output(True).set_dim(shape_o).set_stride(stride_o)\n",
    "stats.set_output(True).set_data_type(cudnn.data_type.BFLOAT16)\n",
    "\n",
    "# serialize the graph\n",
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()\n",
    "serialized_data = graph.serialize()\n",
    "\n",
    "# deserialize the graph\n",
    "deserialized_graph = cudnn.pygraph()\n",
    "deserialized_graph.deserialize(serialized_data)\n",
    "\n",
    "# execute the graph\n",
    "workspace = torch.empty(\n",
    "    deserialized_graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8\n",
    ")\n",
    "variant_pack = {\n",
    "    0: q_gpu,\n",
    "    1: k_gpu,\n",
    "    2: v_gpu,\n",
    "    3: o_gpu,\n",
    "    4: stats_gpu,\n",
    "}\n",
    "deserialized_graph.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the results\n",
    "o_ref = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q_gpu, k_gpu, v_gpu, is_causal=True, scale=attn_scale\n",
    ")\n",
    "torch.testing.assert_close(o_ref, o_gpu, atol=5e-3, rtol=3e-3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
