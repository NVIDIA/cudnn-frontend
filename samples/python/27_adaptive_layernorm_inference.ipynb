{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive LayerNorm: Inference\n",
    "\n",
    "This notebook shows how to compute an adaptive layernorm forward operation using cuDNN.\n",
    "\n",
    "$$\\text{Adaptive\\_LayerNorm}(x) = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot\\gamma+\\beta$$\n",
    "\n",
    "Where $\\mu = E[x]$ and $\\sigma^2 = Var[x]$ are taken over all inputs in a batch, $\\gamma$ and $\\beta$ are learnable parameters and varies for each input in a batch. This is in contrast to the layer norm where $\\gamma$ and $\\beta$ are shared across all inputs in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/27_adaptive_layernorm_inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will apply adaptive layer norm to a tensor of the following shape:\n",
    "\n",
    "- Batch Size: 4\n",
    "- Sequence Size: 1024\n",
    "- Embedding Dimension: 768\n",
    "\n",
    "Let's define these dimensions as constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "print(\"Running with cudnn backend version:\", cudnn.backend_version())\n",
    "\n",
    "handle = cudnn.create_handle()\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "batch, seq_size, embedding_dim = 4, 1024, 768\n",
    "# Epsilon is a small number to prevent division by 0.\n",
    "epsilon_value = 1e-3\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to do inference with adaptive layernorm. This is mostly the same as the forward pass in training except that we are not expecting the mean and variance to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensors\n",
    "x_gpu = torch.randn(batch, seq_size, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "scale_gpu = torch.randn(batch, 1, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "bias_gpu = torch.randn(batch, 1, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "eps_cpu = torch.full((1, 1, 1), epsilon_value, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "# forward pass of adaptive layernorm using cuDNN graph\n",
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\"adaln::input\", \"adaln::scale\", \"adaln::bias\", \"adaln::epsilon\"],\n",
    "    outputs=[\"adaln::Y\"],\n",
    ") as fwd_graph:\n",
    "    out, mean, inv_var = fwd_graph.adalayernorm(\n",
    "        name=\"adaln\",\n",
    "        norm_forward_phase=cudnn.norm_forward_phase.INFERENCE,\n",
    "        input=x_gpu,\n",
    "        scale=scale_gpu,\n",
    "        bias=bias_gpu,\n",
    "        epsilon=eps_cpu,\n",
    "    )\n",
    "    assert mean is None, \"mean should be None in inference mode\"\n",
    "    assert inv_var is None, \"inv_var should be None in inference mode\"\n",
    "    out.set_name(\"output\").set_output(True).set_data_type(dtype)\n",
    "\n",
    "out_gpu = fwd_graph(x_gpu, scale_gpu, bias_gpu, eps_cpu, handle=handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the output by comparing it to the result from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch reference output\n",
    "out_ref = torch.nn.functional.layer_norm(x_gpu, (embedding_dim,), eps=epsilon_value)\n",
    "out_ref = out_ref * scale_gpu + bias_gpu\n",
    "\n",
    "torch.testing.assert_close(out_gpu, out_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Binding APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input tensor GPU buffers. We use PyTorch to allocate GPU tensors so we can reuse them easily when we calculate reference outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate input tensor memory, initialize them to random numbers\n",
    "x_gpu = torch.randn(batch, seq_size, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "scale_gpu = torch.randn(batch, 1, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "bias_gpu = torch.randn(batch, 1, embedding_dim, device=\"cuda\", dtype=dtype)\n",
    "eps_cpu = torch.full((1, 1, 1), epsilon_value, dtype=torch.float32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class UID(Enum):\n",
    "    X = 0\n",
    "    SCALE = 1\n",
    "    BIAS = 2\n",
    "    EPSILON = 3\n",
    "    OUT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cuDNN graph.\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "# Create tensor handles with the graph API, assign UIDs.\n",
    "x = graph.tensor_like(x_gpu.detach()).set_name(\"X\").set_uid(UID.X.value)\n",
    "scale = graph.tensor_like(scale_gpu.detach()).set_name(\"scale\").set_uid(UID.SCALE.value)\n",
    "bias = graph.tensor_like(bias_gpu.detach()).set_name(\"bias\").set_uid(UID.BIAS.value)\n",
    "epsilon = graph.tensor_like(eps_cpu).set_name(\"epsilon\").set_uid(UID.EPSILON.value)\n",
    "\n",
    "# Add a layernorm operation\n",
    "out, _, _ = graph.adalayernorm(\n",
    "    name=\"ADALN\",\n",
    "    input=x,\n",
    "    norm_forward_phase=cudnn.norm_forward_phase.INFERENCE,\n",
    "    scale=scale,\n",
    "    bias=bias,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "# Enable all outputs, by default outputs are disabled\n",
    "out.set_name(\"output\").set_output(True).set_data_type(dtype).set_uid(UID.OUT.value)\n",
    "# print(graph)\n",
    "\n",
    "# Build the graph\n",
    "graph.build([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign UIDs for tensors. UIDs are a unique identifier that will allow us to provide a mapping from tensors created from cuDNN graph api calls, such as `graph.tensor_like()`, to the underlying device memory that will be used to store these tensors. Virtual tensors don't require explicit memory allocated for them, but non-vritual tensors like inputs or outputs will need to have UIDs assigned to them. \n",
    "\n",
    "Alternatively, one can use handles directly in the mapping, however using UIDs can be more convinient for caching of cuDNN graphs.\n",
    "\n",
    "For each of our inputs {X, Scale, Bias, Epsilon} and our output Out, we allocate a UID. \n",
    "\n",
    "After validating and building a cuDNN graph, we can now execute it. To do this, we have to provide input and output buffers. We do this by using the previously allocated UIDs to associate between tensor handles generated from the graph API, and their underlying memory.\n",
    "\n",
    "The desired input values need to be stored in these buffers before the `graph.execute` call. Because we have done a reference computation, we can simply reuse the buffers we have allocated via PyTorch.\n",
    "\n",
    "Note that the EPISLON UID expects a cpu buffer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate output tensor memory using PyTorch\n",
    "out_gpu = torch.empty_like(x_gpu)\n",
    "\n",
    "# Mapping of (UIDs -> memory)\n",
    "variant_pack = {\n",
    "    UID.X.value: x_gpu,\n",
    "    UID.SCALE.value: scale_gpu,\n",
    "    UID.BIAS.value: bias_gpu,\n",
    "    UID.EPSILON.value: eps_cpu,\n",
    "    UID.OUT.value: out_gpu,\n",
    "}\n",
    "\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "graph.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cuDNN's output against PyTorch's and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch reference output\n",
    "out_ref = torch.nn.functional.layer_norm(x_gpu, (embedding_dim,), eps=epsilon_value)\n",
    "out_ref = out_ref * scale_gpu + bias_gpu\n",
    "\n",
    "torch.testing.assert_close(out_gpu, out_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
