{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm Inference Operation\n",
    "\n",
    "This notebook shows how a layernorm operation can be done using cuDNN in inference mode. This is different from the forward pass in training mode that the mean and variance are not computed nor stored.\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot\\gamma+\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/22_layernorm_inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are going to perform the layer norm forward pass with the following problem sizes:\n",
    "\n",
    "- batch size: 4\n",
    "- sequence length: 1024\n",
    "- embedding dimension: 768\n",
    "\n",
    "The tensor will be in 16-bit float format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "handle = cudnn.create_handle()\n",
    "\n",
    "print(\"Running with cudnn backend version:\", cudnn.backend_version())\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "batch, seq_size, embedding_dim = 4, 1024, 768\n",
    "dtype = torch.float16\n",
    "# Epsilon is a small number to prevent division by 0.\n",
    "epsilon_value = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how you can use the `Graph` wrapper to perform forward layer norm with the input tensors in PyTorch.\n",
    "This is highly similar to the one in the [forward pass notebook](20_layernorm_forward.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate random input tensors\n",
    "x_gpu = torch.randn(\n",
    "    batch * seq_size, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype\n",
    ").to(memory_format=torch.channels_last)\n",
    "scale_gpu = torch.randn(1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype).to(\n",
    "    memory_format=torch.channels_last\n",
    ")\n",
    "bias_gpu = torch.randn(1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype).to(\n",
    "    memory_format=torch.channels_last\n",
    ")\n",
    "eps_cpu = torch.full((1, 1, 1, 1), epsilon_value, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "# create a graph\n",
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\"ln::input\", \"ln::scale\", \"ln::bias\", \"ln::epsilon\"],\n",
    "    outputs=[\"ln::Y\"],\n",
    ") as graph:\n",
    "    out, mean, inv_var = graph.layernorm(\n",
    "        name=\"ln\",\n",
    "        input=x_gpu,\n",
    "        norm_forward_phase=cudnn.norm_forward_phase.INFERENCE,\n",
    "        scale=scale_gpu,\n",
    "        bias=bias_gpu,\n",
    "        epsilon=eps_cpu,\n",
    "    )\n",
    "    out.set_output(True).set_data_type(dtype)\n",
    "    assert mean is None, \"Expecting mean to be None under inference mode\"\n",
    "    assert inv_var is None, \"Expecting inv_var to be None under inference mode\"\n",
    "\n",
    "# execute the graph and retrieve the output tensors\n",
    "out_gpu = graph(x_gpu, scale_gpu, bias_gpu, eps_cpu, handle=handle)\n",
    "\n",
    "# verify the result with PyTorch operations\n",
    "out_ref = torch.nn.functional.layer_norm(\n",
    "    x_gpu,\n",
    "    [embedding_dim, 1, 1],\n",
    "    weight=scale_gpu.squeeze(0),\n",
    "    bias=bias_gpu.squeeze(0),\n",
    "    eps=epsilon_value,\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(out_gpu, out_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the one in the [forward pass notebook](20_layernorm_forward.ipynb), the argument `norm_forward_phase` is set to `cudnn.norm_forward_phase.INFERENCE` instead of `cudnn.norm_forward_phase.TRAINING`. The result of this is that the mean and variance are not computed nor stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Binding APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input tensor GPU buffers. We use PyTorch to allocate GPU tensors so we can reuse them to calculate a reference value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate input tensor memory, initialize them to random numbers\n",
    "x_gpu = torch.randn(\n",
    "    batch * seq_size,\n",
    "    embedding_dim,\n",
    "    1,\n",
    "    1,\n",
    "    dtype=dtype,\n",
    "    requires_grad=True,\n",
    "    device=\"cuda\",\n",
    ").to(memory_format=torch.channels_last)\n",
    "scale_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, dtype=dtype, requires_grad=True, device=\"cuda\"\n",
    ").to(memory_format=torch.channels_last)\n",
    "bias_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, dtype=dtype, requires_grad=True, device=\"cuda\"\n",
    ").to(memory_format=torch.channels_last)\n",
    "\n",
    "# Epsilon must be a scalar value on the cpu.\n",
    "epsilon_cpu = torch.full(\n",
    "    (1, 1, 1, 1), epsilon_value, dtype=torch.float32, requires_grad=False, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create reference computation and allocate output tensor GPU buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reference computation outputs here so we can use .empty_like() to create our output buffers\n",
    "out_expected = torch.nn.functional.layer_norm(\n",
    "    x_gpu,\n",
    "    [embedding_dim, 1, 1],\n",
    "    weight=scale_gpu.squeeze(0),\n",
    "    bias=bias_gpu.squeeze(0),\n",
    "    eps=epsilon_value,\n",
    ")\n",
    "\n",
    "\n",
    "# Allocate output tensor memory using PyTorch\n",
    "out_gpu = torch.empty_like(out_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create cuDNN graph and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cuDNN graph\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "# Create tensor handles with the graph API\n",
    "x = graph.tensor_like(x_gpu.detach()).set_name(\"X\")\n",
    "scale = graph.tensor_like(scale_gpu.detach()).set_name(\"scale\")\n",
    "bias = graph.tensor_like(bias_gpu.detach()).set_name(\"bias\")\n",
    "epsilon = graph.tensor_like(epsilon_cpu).set_name(\"epsilon\")\n",
    "\n",
    "(out, mean, inv_var) = graph.layernorm(\n",
    "    name=\"layernorm\",\n",
    "    input=x,\n",
    "    norm_forward_phase=cudnn.norm_forward_phase.INFERENCE,  # Note INFERENCE and not TRAINING\n",
    "    scale=scale,\n",
    "    bias=bias,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "# Enable only the desired output, by default, outputs are disabled\n",
    "out.set_name(\"output\").set_output(True).set_data_type(out_expected.dtype)\n",
    "\n",
    "# Because we have set the norm_forward_phase to INFERENCE, these outputs will be None.\n",
    "assert mean is None\n",
    "assert inv_var is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph.build([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "\n",
    "# To run this block more than once, we need to re-run the previous block to get a new graph.\n",
    "# The same instance of a graph should not be built twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of (handles -> memory)\n",
    "variant_pack = {\n",
    "    x: x_gpu.detach(),\n",
    "    scale: scale_gpu.detach(),\n",
    "    bias: bias_gpu.detach(),\n",
    "    epsilon: epsilon_cpu,\n",
    "    out: out_gpu,\n",
    "}\n",
    "\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "graph.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cuDNN's output against PyTorch's and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference output\n",
    "torch.testing.assert_close(out_gpu, out_expected, rtol=5e-3, atol=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
