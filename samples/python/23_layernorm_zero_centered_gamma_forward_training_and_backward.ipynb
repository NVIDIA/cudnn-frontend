{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm with Zero Centered Gamma: Forward and Backward\n",
    "\n",
    "This notebook shows how to compute a zero centered gamma layernorm forward training and backward operation using cuDNN.\n",
    "\n",
    "$$\\text{LayerNorm\\_Zero\\_Centered\\_Gamma}(x) = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot(1+\\gamma)+\\beta$$\n",
    "\n",
    "Where $\\mu = E[x]$ and $\\sigma^2 = Var[x]$ are taken over all inputs in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/23_layernorm_zero_centered_gamma_forward_training_and_backward.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will apply layer norm to a tensor of the following shape:\n",
    "\n",
    "- Batch Size: 4\n",
    "- Sequence Size: 1024\n",
    "- Embedding Dimension: 768\n",
    "\n",
    "Let's define these dimensions as constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "handle = cudnn.create_handle()\n",
    "print(\"Running with cudnn backend version:\", cudnn.backend_version())\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "assert (\n",
    "    cudnn.backend_version() >= 91000\n",
    "), \"LayerNorm Zero Centered Gamma operation is only supported cuDNN version 9.10.0 or above\"\n",
    "\n",
    "batch, seq_size, embedding_dim = 4, 1024, 768\n",
    "# Epsilon is a small number to prevent division by 0.\n",
    "epsilon_value = 1e-3\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensors\n",
    "x_gpu = torch.randn(\n",
    "    batch * seq_size,\n",
    "    embedding_dim,\n",
    "    1,\n",
    "    1,\n",
    "    device=\"cuda\",\n",
    "    dtype=dtype,\n",
    "    requires_grad=True,\n",
    ").to(memory_format=torch.channels_last)\n",
    "gamma_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype, requires_grad=True\n",
    ").to(memory_format=torch.channels_last)\n",
    "bias_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype, requires_grad=True\n",
    ").to(memory_format=torch.channels_last)\n",
    "one_cpu = torch.ones(1, 1, 1, 1, dtype=torch.float32, device=\"cpu\")\n",
    "eps_cpu = torch.full((1, 1, 1, 1), epsilon_value, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "# forward pass of layernorm using cuDNN graph\n",
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\n",
    "        \"gamma_plus_one::a\",\n",
    "        \"gamma_plus_one::b\",\n",
    "        \"ln_fwd::input\",\n",
    "        \"ln_fwd::bias\",\n",
    "        \"ln_fwd::epsilon\",\n",
    "    ],\n",
    "    outputs=[\"ln_fwd::Y\", \"ln_fwd::MEAN\", \"ln_fwd::INV_VARIANCE\"],\n",
    ") as fwd_graph:\n",
    "    # a pointwise add operation for zero centered gamma + 1\n",
    "    scale = fwd_graph.add(\n",
    "        name=\"gamma_plus_one\",\n",
    "        a=gamma_gpu,\n",
    "        b=one_cpu,\n",
    "    )\n",
    "    # Add a layernorm operation\n",
    "    out, mean, inv_var = fwd_graph.layernorm(\n",
    "        name=\"ln_fwd\",\n",
    "        norm_forward_phase=cudnn.norm_forward_phase.TRAINING,\n",
    "        input=x_gpu,\n",
    "        scale=scale,\n",
    "        bias=bias_gpu,\n",
    "        epsilon=eps_cpu,\n",
    "    )\n",
    "    # Enable all outputs, by default outputs are disabled\n",
    "    # mean and inv_var must be float32 tensors\n",
    "    out.set_name(\"output\").set_output(True).set_data_type(dtype)\n",
    "    mean.set_name(\"mean\").set_output(True).set_data_type(torch.float32)\n",
    "    inv_var.set_name(\"inv_var\").set_output(True).set_data_type(torch.float32)\n",
    "\n",
    "out_gpu, mean_gpu, inv_var_gpu = fwd_graph(\n",
    "    gamma_gpu, one_cpu, x_gpu, bias_gpu, eps_cpu, handle=handle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer norm in cuDNN accepts `scale` and `bias` as input arguments. The `scale` is a factor to multiply to the normalized input. If the scale factor is zero centered, you need a conversion before passing it to the layer norm. The node `gamma_plus_one` defined above is such a conversion.\n",
    "\n",
    "Now, let's compare the output from cuDNN with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch reference output\n",
    "out_ref = torch.nn.functional.layer_norm(\n",
    "    x_gpu,\n",
    "    [embedding_dim, 1, 1],\n",
    "    weight=(1 + gamma_gpu).squeeze(0),\n",
    "    bias=bias_gpu.squeeze(0),\n",
    "    eps=epsilon_value,\n",
    ")\n",
    "mean_ref = x_gpu.float().mean(dim=(1, 2, 3), keepdim=True)\n",
    "inv_var_ref = torch.rsqrt(\n",
    "    torch.var(x_gpu.float(), dim=(1, 2, 3), keepdim=True) + epsilon_value\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(out_gpu, out_ref, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(mean_gpu, mean_ref, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(inv_var_gpu, inv_var_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above output, let's implement the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients: Ask PyTorch not to discard the grads after use so that we can read it twice\n",
    "# out_ref.grad will be used in the cudnn graph, x_gpu.grad, scale_gpu.grad, and bias_gpu.grad will\n",
    "# be used to compare with the cudnn graph output.\n",
    "target = torch.randn_like(out_ref)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(out_ref, target)\n",
    "\n",
    "out_ref.retain_grad()\n",
    "x_gpu.retain_grad()\n",
    "gamma_gpu.retain_grad()\n",
    "bias_gpu.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Backward pass\n",
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\n",
    "        \"gamma_plus_one::a\",\n",
    "        \"gamma_plus_one::b\",\n",
    "        \"ln_bwd::grad\",\n",
    "        \"ln_bwd::input\",\n",
    "        \"ln_bwd::mean\",\n",
    "        \"ln_bwd::inv_variance\",\n",
    "    ],\n",
    "    outputs=[\"ln_bwd::DX\", \"ln_bwd::DSCALE\", \"ln_bwd::DBIAS\"],\n",
    ") as bwd_graph:\n",
    "    scale_bwd = bwd_graph.add(\n",
    "        name=\"gamma_plus_one\",\n",
    "        a=gamma_gpu,\n",
    "        b=one_cpu,\n",
    "    )\n",
    "    dx, dscale, dbias = bwd_graph.layernorm_backward(\n",
    "        name=\"ln_bwd\",\n",
    "        grad=out_ref.grad,\n",
    "        input=x_gpu,\n",
    "        scale=scale_bwd,\n",
    "        mean=mean_gpu,\n",
    "        inv_variance=inv_var_gpu,\n",
    "    )\n",
    "    dx.set_output(True).set_data_type(dtype)\n",
    "    dscale.set_output(True).set_data_type(dtype)\n",
    "    dbias.set_output(True).set_data_type(dtype)\n",
    "\n",
    "dx_gpu, dscale_gpu, dbias_gpu = bwd_graph(\n",
    "    gamma_gpu, one_cpu, out_ref.grad, x_gpu, mean_gpu, inv_var_gpu, handle=handle\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(x_gpu.grad, dx_gpu, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(gamma_gpu.grad, dscale_gpu, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(bias_gpu.grad, dbias_gpu, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Binding APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create GPU buffers as input, the `gamma_gpu` tensor is the zero-centered scale factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype, requires_grad=True\n",
    ").to(memory_format=torch.channels_last)\n",
    "bias_gpu = torch.randn(\n",
    "    1, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype, requires_grad=True\n",
    ").to(memory_format=torch.channels_last)\n",
    "x_gpu = torch.randn(\n",
    "    batch * seq_size,\n",
    "    embedding_dim,\n",
    "    1,\n",
    "    1,\n",
    "    device=\"cuda\",\n",
    "    dtype=dtype,\n",
    "    requires_grad=True,\n",
    ").to(memory_format=torch.channels_last)\n",
    "one_cpu = torch.ones(1, 1, 1, 1, dtype=torch.float32, device=\"cpu\")\n",
    "eps_cpu = torch.full((1, 1, 1, 1), epsilon_value, dtype=torch.float32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create the graph for forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class UID(Enum):\n",
    "    SCALE0 = 1\n",
    "    X = 2\n",
    "    BIAS = 3\n",
    "    OUT = 5\n",
    "    MEAN = 6\n",
    "    INV_VAR = 7\n",
    "    ONE = 8\n",
    "    EPSILON = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cuDNN graph.\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "# Create tensor handles with the graph API, assign UIDs.\n",
    "x = graph.tensor_like(x_gpu.detach()).set_name(\"X\").set_uid(UID.X.value)\n",
    "gamma = (\n",
    "    graph.tensor_like(gamma_gpu.detach()).set_name(\"scale0\").set_uid(UID.SCALE0.value)\n",
    ")\n",
    "one = graph.tensor_like(one_cpu).set_name(\"one\").set_uid(UID.ONE.value)\n",
    "bias = graph.tensor_like(bias_gpu.detach()).set_name(\"bias\").set_uid(UID.BIAS.value)\n",
    "epsilon = graph.tensor_like(eps_cpu).set_name(\"epsilon\").set_uid(UID.EPSILON.value)\n",
    "\n",
    "# A node for pointwise add operation: zero centered gamma + 1\n",
    "scale = graph.add(name=\"gamma_plus_one\", a=gamma, b=one)\n",
    "\n",
    "# A node for layernorm operation\n",
    "out, mean, inv_var = graph.layernorm(\n",
    "    name=\"layernorm\",\n",
    "    norm_forward_phase=cudnn.norm_forward_phase.TRAINING,\n",
    "    input=x,\n",
    "    scale=scale,\n",
    "    bias=bias,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "\n",
    "# Mark output tensors\n",
    "out.set_name(\"output\").set_output(True).set_data_type(dtype).set_uid(UID.OUT.value)\n",
    "mean.set_name(\"mean\").set_output(True).set_data_type(torch.float32).set_uid(\n",
    "    UID.MEAN.value\n",
    ")\n",
    "inv_var.set_name(\"inv_var\").set_output(True).set_data_type(torch.float32).set_uid(\n",
    "    UID.INV_VAR.value\n",
    ")\n",
    "\n",
    "# Build the graph\n",
    "graph.build([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign UIDs for tensors. UIDs are a unique identifier that will allow us to provide a mapping from tensors created from cuDNN graph api calls, such as `graph.tensor_like()`, to the underlying device memory that will be used to store these tensors. Virtual tensors don't require explicit memory allocated for them, but non-vritual tensors like inputs or outputs will need to have UIDs assigned to them. \n",
    "\n",
    "Alternatively, one can use handles directly in the mapping, however using UIDs can be more convinient for caching of cuDNN graphs.\n",
    "\n",
    "For each of our inputs {X, Scale, Bias, Epsilon} and our outputs {Out, Mean, Inverse Variance}, we allocate a UID. \n",
    "\n",
    "After validating and building a cuDNN graph,  we can now execute it. To do this, we have to provide input and output buffers. We do this by using the previously allocated UIDs to associate between tensor handles generated from the graph API, and their underlying memory. \n",
    "\n",
    "The desired input values need to be stored in these buffers before the `graph.execute` call. Because we have done a reference computation, we can simply reuse the buffers we have allocated via PyTorch.\n",
    "\n",
    "Note that the EPISLON UID expects a cpu buffer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_gpu = torch.empty(batch * seq_size, embedding_dim, 1, 1, device=\"cuda\", dtype=dtype)\n",
    "mean_gpu = torch.empty(batch * seq_size, 1, 1, 1, device=\"cuda\", dtype=torch.float32)\n",
    "inv_var_gpu = torch.empty(batch * seq_size, 1, 1, 1, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# Mapping of (UIDs -> memory)\n",
    "variant_pack = {\n",
    "    UID.X.value: x_gpu,\n",
    "    UID.SCALE0.value: gamma_gpu,\n",
    "    UID.BIAS.value: bias_gpu,\n",
    "    UID.EPSILON.value: eps_cpu,\n",
    "    UID.OUT.value: out_gpu,\n",
    "    UID.MEAN.value: mean_gpu,\n",
    "    UID.INV_VAR.value: inv_var_gpu,\n",
    "    UID.ONE.value: one_cpu,\n",
    "}\n",
    "\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "graph.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cuDNN's output against PyTorch's and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch reference output\n",
    "out_ref = torch.nn.functional.layer_norm(\n",
    "    x_gpu,\n",
    "    [embedding_dim, 1, 1],\n",
    "    weight=(1 + gamma_gpu).squeeze(0),\n",
    "    bias=bias_gpu.squeeze(0),\n",
    "    eps=epsilon_value,\n",
    ")\n",
    "mean_ref = x_gpu.float().mean(dim=(1, 2, 3), keepdim=True)\n",
    "inv_var_ref = torch.rsqrt(\n",
    "    torch.var(x_gpu.float(), dim=(1, 2, 3), keepdim=True) + epsilon_value\n",
    ")\n",
    "\n",
    "# compare to reference output\n",
    "torch.testing.assert_close(out_gpu, out_ref, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(mean_gpu, mean_ref, atol=5e-3, rtol=3e-3)\n",
    "torch.testing.assert_close(inv_var_gpu, inv_var_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use random values as groundtruth to calculate the loss and run backward pass in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference backward operation using PyTorch\n",
    "target = torch.randn_like(out_ref)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(out_ref, target)\n",
    "\n",
    "out_ref.retain_grad()\n",
    "x_gpu.retain_grad()\n",
    "gamma_gpu.retain_grad()\n",
    "bias_gpu.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create the backward graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "# Create tensors associated with the backwards graph.\n",
    "# DO NOT reuse tensor handles from the forward graph because tensors are not shared across graphs.\n",
    "d_out = bwd_graph.tensor(\n",
    "    name=\"d_out\", dim=x_gpu.size(), stride=x_gpu.stride(), data_type=x_gpu.dtype\n",
    ")\n",
    "x_bwd = bwd_graph.tensor_like(x, name=\"x\")\n",
    "gamma_bwd = bwd_graph.tensor_like(gamma, name=\"gamma\")\n",
    "one_bwd = graph.tensor_like(one_cpu).set_name(\"one\")\n",
    "mean_bwd = bwd_graph.tensor_like(mean, name=\"mean\")\n",
    "inv_var_bwd = bwd_graph.tensor_like(inv_var, name=\"inv_var\")\n",
    "\n",
    "# A node for pointwise add operation: zero centered gamma + 1\n",
    "scale_bwd = bwd_graph.add(name=\"gamma_bwd_plus_one\", a=gamma_bwd, b=one_bwd)\n",
    "\n",
    "# A node for the layernorm backward operation\n",
    "d_x, d_scale, d_bias = bwd_graph.layernorm_backward(\n",
    "    name=\"DLN\",\n",
    "    grad=d_out,\n",
    "    input=x_bwd,\n",
    "    scale=scale_bwd,\n",
    "    mean=mean_bwd,\n",
    "    inv_variance=inv_var_bwd,\n",
    ")\n",
    "\n",
    "# Enable outputs.\n",
    "d_x.set_output(True).set_data_type(x_gpu.dtype)\n",
    "d_scale.set_output(True).set_data_type(x_gpu.dtype)\n",
    "d_bias.set_output(True).set_data_type(x_gpu.dtype)\n",
    "\n",
    "# Build the bwd_graph\n",
    "bwd_graph.build([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the graph and check correctness against PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output buffers for gradients\n",
    "d_x_gpu = torch.empty_like(x_gpu)\n",
    "d_scale_gpu = torch.empty_like(gamma_gpu)\n",
    "d_bias_gpu = torch.empty_like(bias_gpu)\n",
    "\n",
    "# For the inputs of the backwards graph (x_bwd, d_out, scale_bwd, mean_bwd, inv_var_bwd), we use the outputs of the forwards graph. For d_out we use pytorches autograd .grad functionality.\n",
    "variant_pack = {\n",
    "    x_bwd: x_gpu.detach(),\n",
    "    gamma_bwd: gamma_gpu.detach(),\n",
    "    d_out: out_ref.grad,\n",
    "    mean_bwd: mean_gpu.detach(),\n",
    "    inv_var_bwd: inv_var_gpu.detach(),\n",
    "    d_x: d_x_gpu,\n",
    "    d_scale: d_scale_gpu,\n",
    "    d_bias: d_bias_gpu,\n",
    "    one_bwd: one_cpu,\n",
    "}\n",
    "workspace = torch.empty(\n",
    "    bwd_graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8\n",
    ")\n",
    "bwd_graph.execute(variant_pack, workspace, handle=handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "\n",
    "# compare to reference output\n",
    "torch.testing.assert_close(x_gpu.grad, d_x_gpu, atol=2e-4, rtol=2e-4)\n",
    "torch.testing.assert_close(gamma_gpu.grad, d_scale_gpu, atol=2e-4, rtol=2e-4)\n",
    "torch.testing.assert_close(bias_gpu.grad, d_bias_gpu, atol=2e-4, rtol=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
