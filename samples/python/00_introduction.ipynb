{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to cuDNN Frontend Python API\n",
    "This notebook is an introduction to cuDNN FE graph Python API and how to perform a single fprop convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/00_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for running on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cuDNN Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use cuDNN through the frontend API is to use the `Graph` wrapper object. Below is an example of how to perform a single fprop convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "\n",
    "print(cudnn.backend_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`backend_version()` returns an integer representing the cudnn backend version, e.g. 90000. You can use this to check if the cuDNN backend version supports the operations you need.\n",
    "\n",
    "In the following, we will use PyTorch to hold a random tensor on the GPU and then use the cuDNN to perform a convolution operation. Let's create the tensors in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "handle = cudnn.create_handle()\n",
    "\n",
    "# Create tensor in NHWC format then permute to NCHW\n",
    "X_gpu = torch.randn(8, 56, 56, 64, device=device, dtype=torch.float16).permute(\n",
    "    0, 3, 1, 2\n",
    ")\n",
    "W_gpu = torch.randn(32, 3, 3, 64, device=device, dtype=torch.float16).permute(\n",
    "    0, 3, 1, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates two PyTorch tensors in GPU, with [physical layout in NHWC](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/) but logical layout in NCHW. This is the format [expected by cuDNN](https://docs.nvidia.com/deeplearning/cudnn/frontend/latest/developer/core-concepts.html#tensor-descriptor).\n",
    "\n",
    "`handle` is a pointer to an opaque structure holding the cuDNN library context. The cuDNN library context must be created using `create_handle()` and the returned handle must be passed to all subsequent library function calls as needed. The context should be destroyed at the end using `destroy_handle()`. \n",
    "\n",
    "The context is associated with only one GPU device, the current device at the time of the call to `create_handle()`. However, multiple contexts can be created on the same GPU device.\n",
    "\n",
    "`handle` is used during the execute and to determine which GPU the kernel should be launched. \n",
    "\n",
    "The next step is to create a `pygraph` object so that a graph can be created:\n",
    "\n",
    "Then we can create a cuDNN graph for convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cudnn.Graph(\n",
    "    inputs=[\"conv2d::image\", \"conv2d::weight\"],\n",
    "    outputs=[\"conv_out\"],\n",
    ") as graph:\n",
    "    Y = graph.conv_fprop(\n",
    "        image=X_gpu,  # referencing tensor layout and type\n",
    "        weight=W_gpu,  # referencing tensor layout and type\n",
    "        padding=[1, 1],\n",
    "        stride=[1, 1],\n",
    "        dilation=[1, 1],\n",
    "        compute_data_type=cudnn.data_type.FLOAT,\n",
    "        name=\"conv2d\",\n",
    "    )\n",
    "    # either set io_data_type in Graph or set_data_type on the output tensor\n",
    "    Y.set_output(True).set_data_type(cudnn.data_type.HALF).set_name(\"conv_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code created a cuDNN graph with a single node of convolution operation. The node is assigned the name `conv2d`. It will consume two input tensors, as `image` and `weight`. The output of this node, held by the variable `Y`, will be referenced by the name `conv_out` and regarded as the output of the graph.\n",
    "\n",
    "The graph is created with a context manager, which will automatically finalize the graph after the block is exited. The graph has been assigned the input and output tuples `[\"conv2d::image\", \"conv2d::weight\"]` and `[\"conv_out\"]` respectively. These means the input will be `image` argument in the node named `conv2d` and then `weight` argument in the same node. The output has the name assigned, hence, alternatively, you can also use that name.\n",
    "\n",
    "There are several features shown above:\n",
    "\n",
    "- `Graph` is the main class for creating a cuDNN graph using the context manager.\n",
    "- once a graph is initialized, you can build the graph by creating nodes one by one\n",
    "- even you provided PyTorch tensor `X_gpu` as argument `image` to the node `conv2d`, it is only used as a placeholder to define the properties of the input tensor (e.g. data type, physical layout, logical layout). The actual data is not used at this stage.\n",
    "- output tensors from a node (cuDNN tensor object) should be defined as output using `set_output(True)` to indicate that the graph should hold the reference to the tensor to return to the user\n",
    "- cuDNN graph is flexible, hence you would need to provide additional information to set up the graph, such as the data type of the output from a node, if there is an ambiguity.\n",
    "\n",
    "Once you finish defining the graph, you can execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_gpu = graph(\n",
    "    X_gpu, W_gpu, handle=handle\n",
    ")  # reading the tensor value to execute the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is called like a function. The input arguments are the actual tensors to be used and the order of the input arguments is as defined using `set_io_tuples()`. The output from the graph is a PyTorch tensor created dynamically.\n",
    "\n",
    "Note that we used `X_gpu` and `W_gpu` when we defined the graph previously and reused them here. It is not necessary. But you should make sure the tensors you used to define the graph are compatible (data type and layouts) with the tensors you invoked the graph with.\n",
    "\n",
    "To verify that the graph is working correctly, you can check the result with that generated by PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ref = torch.nn.functional.conv2d(X_gpu, W_gpu, padding=1)\n",
    "torch.testing.assert_close(Y_gpu, Y_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same input tensors `X_gpu` and `W_gpu`, you can tell that the result from cuDNN `Y_gpu` and the result from PyTorch `Y_ref` are numerically close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cuDNN Frontend Python Bindings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `pygraph` object so that a graph can be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    name=\"cudnn_graph_0\",\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `pygraph` object is the subgraph that is provided to the cuDNN for execution.\n",
    "\n",
    "The arguments you provided to create the `pygraph` object are optional:\n",
    "\n",
    "- You can assign a `name` to the graph for future reference.\n",
    "- The `io_data_type` provides the data type of the input and output tensors of the graph. This can be overridden by actual tensor data type.\n",
    "- The `compute_data_type` provides the data type in which computation will happen. This can be overridden by actual compute data type of the individual operation.\n",
    "\n",
    "With the `pygraph` object created, you can define nodes in this graph. All tensors used by a node should be specified as a cuDNN `tensor` object. For the convolution operation in the previous example, we need to create two tensors, `X` and `W`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = graph.tensor(\n",
    "    name=\"X\",\n",
    "    dim=[8, 64, 56, 56],\n",
    "    stride=[56 * 56 * 64, 1, 56 * 64, 64],\n",
    "    data_type=cudnn.data_type.HALF,\n",
    ")\n",
    "W = graph.tensor(name=\"W\", dim=[32, 64, 3, 3], stride=[3 * 3 * 64, 1, 3 * 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`graph.tensor` creates an entry edge to the graph. The main attributes of the tensor class are `dim`, `stride` and `data_type`. Some other attributes are `is_virtual` (mainly used for interior nodes in graph), `is_pass_by_value` for scalar tensors. Assigning a `name` to the tensor is optional.\n",
    "\n",
    "Note that the `W` tensor above was created without `data_type`. Its data type is deduced from the `io_data_type` of the `pygraph` object that was used to create the tensor.\n",
    "\n",
    "Next is the convolution node, which uses the cuDNN tensors `X` and `W` as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = graph.conv_fprop(\n",
    "    X,\n",
    "    W,\n",
    "    padding=[1, 1],\n",
    "    stride=[1, 1],\n",
    "    dilation=[1, 1],\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a *convolution forward* operation with padding as `[1,1]` on the input `X` tensor. You can run `help (cudnn.pygraph.conv_fprop)` to see explanation of the other parameters, `compute_data_type`, `stride`, `dilation`.\n",
    "\n",
    "Note that when you use the Python binding directly, you must provide `X` and `W` as cuDNN tensors. Using other tensors, such as PyTorch tensors, is not allowed.\n",
    "\n",
    "The output of the convolution node above is a cuDNN tensor. Since you want to read from it, you should mark it as output using `set_output(True)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.set_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the output of any operation is *virtual* (does not have device pointer associated). This is because the output can be fed as input to the next operation in graph. In order to terminate the graph, or to mark the tensor *non-virtual* we need to set it as output. Multiple tensors can be marked as output.\n",
    "\n",
    "At this point, the graph is defined. You should finalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.build([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "# print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following things happen in the above call:\n",
    "\n",
    "- Validation of inputs, outputs and output shape deduction.\n",
    "- Lowering pass into the cuDNN dialect.\n",
    "- Heuristics query to determine which execution plan to run.\n",
    "- Runtime compilation of the plan if needed\n",
    "\n",
    "This function can be split into its constituents to give you a better control over each phase.\n",
    "\n",
    "Once the graph is built, you can use `print()` to inspect the graph after the shape and datatype deduction. For example, the tensor `Y` above as the output from the convolution will have its shape and data type known only after the graph is built.\n",
    "\n",
    "You need to provide the actual data to execute the graph. Let's use PyTorch to create a few random tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpu = torch.randn(\n",
    "    8, 64, 56, 56, requires_grad=False, device=\"cuda\", dtype=torch.float16\n",
    ").to(memory_format=torch.channels_last)\n",
    "W_gpu = torch.randn(\n",
    "    32, 64, 3, 3, requires_grad=False, device=\"cuda\", dtype=torch.float16\n",
    ").to(memory_format=torch.channels_last)\n",
    "Y_gpu = torch.zeros(\n",
    "    8, 32, 56, 56, requires_grad=False, device=\"cuda\", dtype=torch.float16\n",
    ").to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tensors reside in the GPU. They are in the \"channel last\" physical layout (required for cuDNN convolution operations). You are not required to use PyTorch; cuDNN also supports other dlpack tensors on the GPU.\n",
    "\n",
    "To execute the graph, you need to create a workspace and provide a mapping between the cuDNN tensors and the actual allocated tensors (the \"variant pack\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "variant_pack = {X: X_gpu, W: W_gpu, Y: Y_gpu}\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workspace is a buffer that the graph can use during execution. You should allocate a piece of memory of sufficiently large on the GPU. The size can be computed from the graph using `get_workspace_size()`.\n",
    "\n",
    "The execute call launches the kernel for execution on the GPU device. The output, which `Y` was assigned to, will be populated to `Y_gpu`.\n",
    "\n",
    "You can verify the result with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ref = torch.nn.functional.conv2d(X_gpu, W_gpu, padding=1)\n",
    "torch.testing.assert_close(Y_gpu, Y_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can destroy the handle when you no longer need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
