{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "This notebook demonstrates how to use cuDNN's matrix multiplication API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/10_matrix_multiplication.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for running on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires an NVIDIA GPU. If `nvidia-smi` fails, go to Runtime -> Change runtime type -> Hardware accelerator and confirm a GPU is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab, you will need to install the cudnn python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication with Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example shown in [a previous notebook](02_binding.ipynb). It's repeated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "handle = cudnn.create_handle()\n",
    "\n",
    "dtype = torch.float16\n",
    "B, M, N, K = 16, 128, 128, 512\n",
    "\n",
    "# input tensors\n",
    "a_gpu = torch.randn(B, M, K, device=device, dtype=dtype)\n",
    "b_gpu = torch.randn(B, K, N, device=device, dtype=dtype)\n",
    "d_gpu = torch.randn(1, M, N, device=device, dtype=dtype)\n",
    "\n",
    "# reference output\n",
    "c_ref = torch.matmul(a_gpu, b_gpu) + d_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the `Graph` wrapper to create a graph for matrix multiplication. The graph is executed as a function after the input and output order is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\"mm::A\", \"mm::B\", \"bias::bias\"],\n",
    "    outputs=[\"bias::OUT_0\"],\n",
    ") as graph:\n",
    "    AB = graph.matmul(\n",
    "        name=\"mm\",\n",
    "        A=a_gpu,\n",
    "        B=b_gpu,\n",
    "    )\n",
    "    C = graph.bias(name=\"bias\", input=AB, bias=d_gpu)\n",
    "    C.set_output(True)\n",
    "\n",
    "c_gpu = graph(a_gpu, b_gpu, d_gpu, handle=handle)\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python binding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equivalent using the Python binding APIs. It is more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle and construct the graph\n",
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "d_cudnn = graph.tensor_like(d_gpu)\n",
    "\n",
    "ab = graph.matmul(name=\"mm\", A=a_cudnn, B=b_cudnn)\n",
    "c_cudnn = graph.bias(name=\"bias\", input=ab, bias=d_cudnn)\n",
    "c_cudnn.set_output(True)\n",
    "\n",
    "# build and validate the graph\n",
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()\n",
    "\n",
    "# place holder for cuDNN output\n",
    "c_gpu = torch.empty(B, M, N, device=device, dtype=dtype)\n",
    "\n",
    "# execute the graph\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=device, dtype=torch.uint8)\n",
    "variant_pack = {\n",
    "    a_cudnn: a_gpu,  # input\n",
    "    b_cudnn: b_gpu,  # input\n",
    "    d_cudnn: d_gpu,  # input\n",
    "    c_cudnn: c_gpu,  # output\n",
    "}\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication with Mixed Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication operation is supposed to be done in the same precision. To multiply two matrices of different precisions, you need to cast the precision first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, M, N, K = 16, 128, 128, 512\n",
    "\n",
    "a_gpu = torch.randint(4, (B, M, K), device=device, dtype=torch.int8)\n",
    "b_gpu = torch.randn(B, K, N, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "c_ref = torch.matmul(a_gpu.to(torch.bfloat16), b_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to create a graph for matrix multiplication with mixed precision. It involves two nodes: one for precision casting and another for actual matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\"iden::input\", \"mm::B\"],\n",
    "    outputs=[\"mm::C\"],\n",
    ") as graph:\n",
    "    # cast \"A\" to same data type as \"B\"\n",
    "    a_casted = graph.identity(\n",
    "        name=\"iden\", input=a_gpu, compute_data_type=cudnn.data_type.FLOAT\n",
    "    )\n",
    "    a_casted.set_data_type(torch.bfloat16)\n",
    "    # matmul with two tensors of same data type\n",
    "    c = graph.matmul(\n",
    "        name=\"mm\", A=a_casted, B=b_gpu, compute_data_type=cudnn.data_type.FLOAT\n",
    "    )\n",
    "    c.set_output(True).set_data_type(torch.bfloat16)\n",
    "\n",
    "c_gpu = graph(a_gpu, b_gpu, handle=handle)\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python binding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent code using the Python binding APIs is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph\n",
    "graph = cudnn.pygraph()\n",
    "\n",
    "a_cudnn = graph.tensor_like(a_gpu)\n",
    "b_cudnn = graph.tensor_like(b_gpu)\n",
    "\n",
    "a_casted = graph.identity(\n",
    "    input=a_cudnn,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "a_casted.set_data_type(cudnn.data_type.BFLOAT16)\n",
    "c_cudnn = graph.matmul(\n",
    "    name=\"matmul\",\n",
    "    A=a_casted,\n",
    "    B=b_cudnn,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "c_cudnn.set_output(True).set_data_type(cudnn.data_type.BFLOAT16)\n",
    "\n",
    "# validate and build\n",
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()\n",
    "\n",
    "# execute the graph\n",
    "c_gpu = torch.randn(B, M, N, device=device, dtype=torch.bfloat16)\n",
    "variant_pack = {\n",
    "    a_cudnn: a_gpu,\n",
    "    b_cudnn: b_gpu,\n",
    "    c_cudnn: c_gpu,\n",
    "}\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "graph.execute(variant_pack, workspace)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# verify the result\n",
    "torch.testing.assert_close(c_gpu, c_ref, atol=5e-3, rtol=3e-3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
