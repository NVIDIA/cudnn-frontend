{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot Product Attention (SDPA) in cuDNN Frontend\n",
    "\n",
    "This notebook is an example for the scaled dot product attention operator in cuDNN frontend. This operation computes scaled dot product attention as\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V$$\n",
    "\n",
    "using the FlashAttention-2 algorithm described in the paper [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691). It is applicable for both training and inference phases, with an option to generate a stats tensor to be used for backwards training computation.\n",
    "\n",
    "The full documentation can be found in: [docs/operations/Attention.md#scaled-dot-product-attention](https://github.com/NVIDIA/cudnn-frontend/blob/main/docs/operations/Attention.md#scaled-dot-product-attention)\n",
    "\n",
    "The python test code for the full set of features can be found in: [test/python/test_mhas.py](https://github.com/NVIDIA/cudnn-frontend/blob/main/test/python/test_mhas.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/cudnn-frontend/blob/main/samples/python/50_sdpa_forward.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires an NVIDIA GPU A100 or newer. If running on Colab, go to Runtime → Change runtime type → Hardware accelerator and select a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system('pip install nvidia-cudnn-cu12')\n",
    "# get_ipython().system('pip install nvidia-cudnn-frontend')\n",
    "# get_ipython().system('pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the problem size from the original GPT-2 paper where:\n",
    "\n",
    " - maximum sequence length = 1024\n",
    " - hidden dim = number of heads $\\times$ embedding dimension per head = 12 $\\times$ 64 = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cudnn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "handle = cudnn.create_handle()\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "assert (\n",
    "    torch.cuda.get_device_capability()[0] >= 8\n",
    "), \"SDPA operation is only supported on SM80 architecture (Ampere) or above\"\n",
    "assert (\n",
    "    cudnn.backend_version() >= 8903\n",
    "), \"SDPA operation is only supported cuDNN version 8.9.3 or above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4  # batch size\n",
    "S = 1024  # maximum sequence length\n",
    "H = 12  # query number of heads\n",
    "D = 64  # embedding dimension per head\n",
    "dtype = torch.half\n",
    "\n",
    "attn_scale = 1.0 / math.sqrt(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate random input tensors: Should be in BSHD physical layout and BHSD logical layout\n",
    "q_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)\n",
    "k_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)\n",
    "v_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)\n",
    "\n",
    "# create a graph\n",
    "with cudnn.Graph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    "    inputs=[\"SDPA::q\", \"SDPA::k\", \"SDPA::v\"],\n",
    "    outputs=[\"attn_output\"],\n",
    ") as graph:\n",
    "    o, _ = graph.sdpa(\n",
    "        name=\"SDPA\",\n",
    "        q=q_gpu,\n",
    "        k=k_gpu,\n",
    "        v=v_gpu,\n",
    "        attn_scale=attn_scale,\n",
    "        is_inference=True,\n",
    "        use_causal_mask=True,\n",
    "    )\n",
    "    o.set_output(True).set_name(\"attn_output\").set_dim(q_gpu.shape).set_stride(\n",
    "        q_gpu.stride()\n",
    "    )\n",
    "\n",
    "# execute the graph\n",
    "o_gpu = graph(q_gpu, k_gpu, v_gpu, handle=handle)\n",
    "\n",
    "# verify the result with PyTorch operations\n",
    "o_ref = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q_gpu, k_gpu, v_gpu, is_causal=True, scale=attn_scale\n",
    ")\n",
    "torch.testing.assert_close(o_ref, o_gpu, atol=5e-3, rtol=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Binding APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the query, key, value, and output GPU tensors using PyTorch. However, the user may use any DLPack compatible tensor instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tensors will have non-interleaved\n",
    "# BHSD (batch, num_head, sequence_length, dims_per_head) logical tensor layout\n",
    "dims = (B, H, S, D)\n",
    "# BSHD (batch, sequence_length, num_head, dims_per_head) physical layout\n",
    "strides = (S * H * D, D, H * D, 1)\n",
    "# For BHSD (batch, num_head, sequence_length, dims_per_head) physical tensor layout, uncomment the following:\n",
    "# strides = (S*H*D, S*D, D, 1)\n",
    "\n",
    "q_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)\n",
    "k_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)\n",
    "v_gpu = torch.randn(B, S, H, D, device=\"cuda\", dtype=dtype).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = cudnn.pygraph(\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    intermediate_data_type=cudnn.data_type.FLOAT,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")\n",
    "\n",
    "q = graph.tensor_like(q_gpu)\n",
    "k = graph.tensor_like(k_gpu)\n",
    "v = graph.tensor_like(v_gpu)\n",
    "\n",
    "# the second return for the stats tensor is used for training only.\n",
    "# causal mask is enabled\n",
    "o, _ = graph.sdpa(\n",
    "    name=\"sdpa\",\n",
    "    q=q,\n",
    "    k=k,\n",
    "    v=v,\n",
    "    generate_stats=False,\n",
    "    attn_scale=attn_scale,\n",
    "    use_causal_mask=True,\n",
    ")\n",
    "\n",
    "o.set_output(True).set_dim(dims).set_stride(strides)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.validate()\n",
    "graph.build_operation_graph()\n",
    "graph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\n",
    "graph.check_support()\n",
    "graph.build_plans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate output tensor\n",
    "o_gpu = torch.empty(B, H, S, D, device=\"cuda\", dtype=dtype).as_strided(dims, strides)\n",
    "\n",
    "# execute the graph\n",
    "variant_pack = {\n",
    "    q: q_gpu,\n",
    "    k: k_gpu,\n",
    "    v: v_gpu,\n",
    "    o: o_gpu,\n",
    "}\n",
    "workspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\n",
    "graph.execute(variant_pack, workspace, handle=handle)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cuDNN's output against PyTorch's and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ref = q_gpu.detach().float().requires_grad_()\n",
    "k_ref = k_gpu.detach().float().requires_grad_()\n",
    "v_ref = v_gpu.detach().float().requires_grad_()\n",
    "\n",
    "o_ref = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q_ref, k_ref, v_ref, is_causal=True, scale=attn_scale\n",
    ")\n",
    "torch.testing.assert_close(o_ref, o_gpu.float(), atol=5e-3, rtol=3e-3)\n",
    "\n",
    "\n",
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
